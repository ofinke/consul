# ChatTask Flow Documentation

## Overview
`ChatTask` is a simple flow for interacting with a language model (LLM) by asking a question and receiving an answer. It is defined in `src/consul/flows/tasks/chat.py` and is part of the Consul framework. This flow is designed for straightforward LLM interactions without tool usage or iterative reasoning steps.

## Class: ChatTask

### Inheritance
- Inherits from: `BaseFlow`

### Purpose
The `ChatTask` class provides a minimal flow for sending a prompt to an LLM and returning its response. It is suitable for use cases where only a single exchange with the LLM is required, such as question answering or simple chat.

## Key Methods and Properties

### `input_schema`, `state_schema`, `output_schema`
- All return `BaseGraphState`.
- Define the schema for input, state, and output of the chat flow.

### `build_system_prompt(self) -> list[ChatMessage]`
- Constructs the system prompt for the LLM using the prompt history from the configuration.
- Each prompt turn is formatted using `PROMPT_FORMAT_MAPPING` and converted into a `ChatMessage`.

### `build_graph(self) -> StateGraph`
- Constructs the execution graph for the chat flow using the `StateGraph` class.
- Defines a single node:
  - **llm_call**: Calls the LLM with the full message history (system prompt + user messages) and appends the LLM's response to the state.
- Sets both the entry and finish point of the graph to `llm_call`, ensuring a single-step interaction.

#### `llm_node(state: BaseGraphState) -> BaseGraphState`
- Internal function used as the node in the graph.
- Combines the system prompt and current messages, invokes the LLM, and updates the state with the response.

## Flow Logic
1. **System Prompt Construction**: The system prompt is built from the configured prompt history.
2. **LLM Call**: The LLM is invoked with the full message history (system prompt + user messages).
3. **Response Handling**: The LLM's response is appended to the message history and returned as the output state.

## Integration
- **Prompts**: System prompts are built from configurable prompt history, allowing flexible chat behavior.
- **No Tool Support**: Unlike agent flows, `ChatTask` does not support tool calls or iterative reasoning.

## Usage
To use `ChatTask`, instantiate it with the appropriate configuration. The flow will handle sending the prompt to the LLM and returning the response in a single step.

## Related Components
- `BaseFlow`, `BaseGraphState`: Provide foundational flow and state management.
- `PROMPT_FORMAT_MAPPING`: Used for formatting prompt turns.
- `StateGraph`: Manages the execution graph for the chat flow.

## Note
This documentation was generated by AI flow 'docs' from Consul program (https://github.com/ofinke/consul) using LLM model gpt-4-1106-preview.
